{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, RepeatVector, Dropout, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# Suppress the TensorFlow warning\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import  PathLineSentences, LineSentence\n",
    "from gensim import utils\n",
    "from time import time\n",
    "from time import time\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../train_data/training-monolingual.tokenized.shuffled-pre-processed/\n"
     ]
    }
   ],
   "source": [
    "folder = 'train_data/training-monolingual.tokenized.shuffled/'\n",
    "folder_original = 'train_data/training-monolingual.tokenized.shuffled/'\n",
    "preprocessed_folder = '../train_data/training-monolingual.tokenized.shuffled-pre-processed/'\n",
    "print(preprocessed_folder)\n",
    "folder_short = 'train_data/preprocessed-short/'\n",
    "folder_full = 'train_data/preprocessed-full/'\n",
    "\n",
    "folder_test = 'train_data/heldout/'\n",
    "# short_folder = 'training-monolingual.tokenized.shuffled_short/'\n",
    "# words_list_file= 'liste_mots_devoir4.txt'\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self,path,files) -> None:\n",
    "        self.path = path\n",
    "        self.files = files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for f in self.files:\n",
    "            for line in open(self.path+f, 'r', encoding=\"utf8\"):\n",
    "                # assume there's one document per line, tokens separated by whitespace\n",
    "                yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(ext,save_model,w2v_model,times,sizes,sent_len):\n",
    "    \n",
    "    modelname = f\"models/{ext}.w2v\"\n",
    "    csv_times_sizes= f\"outputs/{ext}.csv\"\n",
    "\n",
    "    if save_model:\n",
    "        # stream the model\n",
    "        #w2v_model.init_sims(replace=True) # deprecated: calls to init_sims() are unnecessary\n",
    "        w2v_model.save(modelname)\n",
    "        logging.info(f\"saved {modelname}\")\n",
    " \n",
    "    np.savetxt(csv_times_sizes, [times,sizes,sent_len],delimiter =\", \", fmt ='% s')\n",
    "    logging.info(f\"saved {csv_times_sizes}\")\n",
    "    \n",
    "    \n",
    "def train(save_model = True , data = folder,nb_tranches=10,ext='name'):\n",
    "    # Utilisez gensim pour entrainer des representations vectorielles sur tout ou partie du 1BWC.\n",
    "    #train by tranche + save the time for each tranche\n",
    "    print('Starting the training ')\n",
    "    files = listdir(data)[:nb_tranches]\n",
    "    sents = []\n",
    "    times = []\n",
    "    sizes = []\n",
    "    sent_len = []\n",
    "    nb_sentences = 0\n",
    "    allfiles = False\n",
    "    if nb_tranches==99:\n",
    "        allfiles = True\n",
    "        print('all files')\n",
    "\n",
    "    training_sample = \"training_sample.txt\" \n",
    "    myfile = open(\"training_sample.txt\", \"w\")\n",
    "    myfile.close()\n",
    "\n",
    "    for i,fn in enumerate(files):\n",
    "        a = 99 if allfiles else i+1\n",
    "        print(f\"\\n{'#-'*10}-Files 1..{a}:\")\n",
    "        logging.info(f\"\\n{'#-'*10}-Files 1..{a}:\")\n",
    "        t = time()\n",
    "        \n",
    "        # concatenate corpus files because LineSentence() takes only a single file\n",
    "        if not allfiles: \n",
    "            os.system(\"cat \"+data+fn+ \" >> \"+ training_sample)\n",
    "        t = time()\n",
    "        \n",
    "        print(f\"- Temps de lecture de sentences (en secondes): {(time() - t)}\\n\")\n",
    "        logging.info(f\"- Temps de lecture de sentences (en secondes): {(time() - t)}\\n\")\n",
    "        l = len(open(data+fn, 'r', encoding=\"utf8\").readlines())\n",
    "        print(f\"Sentences found : {l}\")\n",
    "        logging.info(f\"Sentences found : {l}\")\n",
    "        nb_sentences+=l\n",
    "        print(f\"Total Sentences processed : {nb_sentences}\")\n",
    "        logging.info(f\"Total Sentences processed : {nb_sentences}\")\n",
    "        sent_len.append(nb_sentences)\n",
    "\n",
    "    sents = PathLineSentences(data) if allfiles else LineSentence(training_sample)  \n",
    "    # Time to build model \n",
    "    t = time()\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences = sents,\n",
    "        min_count=1, window=5,  vector_size=100,negative=5,\n",
    "        min_alpha=1e-4, workers=(os.cpu_count()*2 - 1), sample=0.01, alpha=1e-2, epochs=10\n",
    "        )\n",
    "    end = round((time() - t),2)\n",
    "    times.append(end)\n",
    "    print(f\"- Temps d'entrainement (en secondes): {end}\\n\")\n",
    "\n",
    "    # Size of the model\n",
    "    size = round(w2v_model.estimate_memory()['total']/(1024*1024),2)\n",
    "    sizes.append(size)\n",
    "\n",
    "    print(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "            Total en MB: {size}\")\n",
    "    logging.info(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "            Total en MB: {size}\")\n",
    "\n",
    "    print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "    logging.info(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "    #print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\n",
    "    #w2v_model.init_sims(replace=True)\n",
    "    print(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "    logging.info(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "\n",
    "    \n",
    "\n",
    "    checkpoint(ext,save_model,w2v_model,times,sizes,sent_len)\n",
    "\n",
    "    return times, sent_len, sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training \n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..1:\n",
      "- Temps de lecture de sentences (en secondes): 2.86102294921875e-06\n",
      "\n",
      "Sentences found : 306068\n",
      "Total Sentences processed : 306068\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..2:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 307000\n",
      "Total Sentences processed : 613068\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..3:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305915\n",
      "Total Sentences processed : 918983\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..4:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306362\n",
      "Total Sentences processed : 1225345\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..5:\n",
      "- Temps de lecture de sentences (en secondes): 2.1457672119140625e-06\n",
      "\n",
      "Sentences found : 305714\n",
      "Total Sentences processed : 1531059\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..6:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 305440\n",
      "Total Sentences processed : 1836499\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..7:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 306552\n",
      "Total Sentences processed : 2143051\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..8:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 307045\n",
      "Total Sentences processed : 2450096\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..9:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305917\n",
      "Total Sentences processed : 2756013\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..10:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306380\n",
      "Total Sentences processed : 3062393\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..11:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306290\n",
      "Total Sentences processed : 3368683\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..12:\n",
      "- Temps de lecture de sentences (en secondes): 3.337860107421875e-06\n",
      "\n",
      "Sentences found : 305594\n",
      "Total Sentences processed : 3674277\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..13:\n",
      "- Temps de lecture de sentences (en secondes): 3.337860107421875e-06\n",
      "\n",
      "Sentences found : 305575\n",
      "Total Sentences processed : 3979852\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..14:\n",
      "- Temps de lecture de sentences (en secondes): 2.86102294921875e-06\n",
      "\n",
      "Sentences found : 306408\n",
      "Total Sentences processed : 4286260\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..15:\n",
      "- Temps de lecture de sentences (en secondes): 3.0994415283203125e-06\n",
      "\n",
      "Sentences found : 306329\n",
      "Total Sentences processed : 4592589\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..16:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 306534\n",
      "Total Sentences processed : 4899123\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..17:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306553\n",
      "Total Sentences processed : 5205676\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..18:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306372\n",
      "Total Sentences processed : 5512048\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..19:\n",
      "- Temps de lecture de sentences (en secondes): 8.106231689453125e-06\n",
      "\n",
      "Sentences found : 305591\n",
      "Total Sentences processed : 5817639\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..20:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 305446\n",
      "Total Sentences processed : 6123085\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..21:\n",
      "- Temps de lecture de sentences (en secondes): 4.0531158447265625e-06\n",
      "\n",
      "Sentences found : 306206\n",
      "Total Sentences processed : 6429291\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..22:\n",
      "- Temps de lecture de sentences (en secondes): 3.0994415283203125e-06\n",
      "\n",
      "Sentences found : 306084\n",
      "Total Sentences processed : 6735375\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..23:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 305909\n",
      "Total Sentences processed : 7041284\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..24:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306116\n",
      "Total Sentences processed : 7347400\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..25:\n",
      "- Temps de lecture de sentences (en secondes): 3.0994415283203125e-06\n",
      "\n",
      "Sentences found : 306273\n",
      "Total Sentences processed : 7653673\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..26:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 306324\n",
      "Total Sentences processed : 7959997\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..27:\n",
      "- Temps de lecture de sentences (en secondes): 1.6689300537109375e-06\n",
      "\n",
      "Sentences found : 306804\n",
      "Total Sentences processed : 8266801\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..28:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305485\n",
      "Total Sentences processed : 8572286\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..29:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306680\n",
      "Total Sentences processed : 8878966\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..30:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305807\n",
      "Total Sentences processed : 9184773\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..31:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306259\n",
      "Total Sentences processed : 9491032\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..32:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-05\n",
      "\n",
      "Sentences found : 305639\n",
      "Total Sentences processed : 9796671\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..33:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 306700\n",
      "Total Sentences processed : 10103371\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..34:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305408\n",
      "Total Sentences processed : 10408779\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..35:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 305297\n",
      "Total Sentences processed : 10714076\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..36:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 305511\n",
      "Total Sentences processed : 11019587\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..37:\n",
      "- Temps de lecture de sentences (en secondes): 2.86102294921875e-06\n",
      "\n",
      "Sentences found : 306964\n",
      "Total Sentences processed : 11326551\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..38:\n",
      "- Temps de lecture de sentences (en secondes): 2.1457672119140625e-06\n",
      "\n",
      "Sentences found : 305032\n",
      "Total Sentences processed : 11631583\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..39:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305933\n",
      "Total Sentences processed : 11937516\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..40:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 305644\n",
      "Total Sentences processed : 12243160\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..41:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306092\n",
      "Total Sentences processed : 12549252\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..42:\n",
      "- Temps de lecture de sentences (en secondes): 2.1457672119140625e-06\n",
      "\n",
      "Sentences found : 306879\n",
      "Total Sentences processed : 12856131\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..43:\n",
      "- Temps de lecture de sentences (en secondes): 2.6226043701171875e-06\n",
      "\n",
      "Sentences found : 306300\n",
      "Total Sentences processed : 13162431\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..44:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 305912\n",
      "Total Sentences processed : 13468343\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..45:\n",
      "- Temps de lecture de sentences (en secondes): 2.384185791015625e-06\n",
      "\n",
      "Sentences found : 306088\n",
      "Total Sentences processed : 13774431\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..46:\n",
      "- Temps de lecture de sentences (en secondes): 2.1457672119140625e-06\n",
      "\n",
      "Sentences found : 305308\n",
      "Total Sentences processed : 14079739\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..47:\n",
      "- Temps de lecture de sentences (en secondes): 2.86102294921875e-06\n",
      "\n",
      "Sentences found : 306016\n",
      "Total Sentences processed : 14385755\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..48:\n",
      "- Temps de lecture de sentences (en secondes): 1.9073486328125e-06\n",
      "\n",
      "Sentences found : 305065\n",
      "Total Sentences processed : 14690820\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..49:\n",
      "- Temps de lecture de sentences (en secondes): 1.7642974853515625e-05\n",
      "\n",
      "Sentences found : 306055\n",
      "Total Sentences processed : 14996875\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..50:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Temps de lecture de sentences (en secondes): 3.814697265625e-06\n",
      "\n",
      "Sentences found : 305220\n",
      "Total Sentences processed : 15302095\n",
      "- Temps d'entrainement (en secondes): 2725.63\n",
      "\n",
      "- Taille du modele sur disque (en octets)): {'vocab': 715018000, 'vectors': 572014400, 'syn1neg': 572014400, 'total': 1859046800}\n",
      "            Total en MB: 1772.93\n",
      "\n",
      "- Nombre de mots encodés (= taille du vocab): 1430036\n",
      "\n",
      "[('lesions', 0.8858491778373718), ('tumours', 0.8723723292350769), ('deformities', 0.8477707505226135), ('tumors', 0.8451257348060608), ('mutations', 0.835237979888916), ('cysts', 0.8215368986129761), ('malformations', 0.8200745582580566), ('defects', 0.816528856754303), ('polyps', 0.8129066824913025), ('disorders', 0.8103189468383789)]\n",
      "Total execution time: 45.94 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacem/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/numpy/lib/npyio.py:1378: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.asarray(X)\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "save_model = True \n",
    "times, sent_len, sizes = train(save_model,preprocessed_folder, 50,'model')\n",
    "print(f\"Total execution time: {round((time()-st)/60,2)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = f\"models/model.w2v\"\n",
    "model = Word2Vec.load(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.9015681   -1.4825824    1.0830806   -3.1107342    1.8650055\n",
      "   1.249914    -1.4759218   -4.1782336   -3.1578116   -1.6441156\n",
      "  -0.43455067   0.71476966   1.665312    -3.4507391   -3.0525122\n",
      "   5.592342     1.705296    -0.9882554    0.02499667 -10.047542\n",
      "   1.6611887   -2.2140872   -0.5830067   -2.7428908   -1.9601579\n",
      "  -0.9092333   -1.6673864    3.9021957   -1.0805547    2.2152247\n",
      "   1.3757206    0.40139925   4.455378    -1.4243028    1.1797311\n",
      "  -0.9060254   -1.0316461   -0.6184769   -0.70201856  -0.8036557\n",
      "   3.52521     -2.6057773    0.31820267   1.0761127    2.9087982\n",
      "   0.7554629    0.04572703   2.0367873    1.7707905    1.9300609\n",
      "   1.0439298    1.6404431   -1.751548    -2.2012062   -0.9391908\n",
      "   0.6002678    0.78248096   0.78564596  -2.7882662   -7.830961\n",
      "  -5.0490227    2.6179461    1.478342     1.2702339    2.911088\n",
      "   2.041558    -0.40612686   0.11329196  -2.429788     5.0052986\n",
      "   3.054289    -0.26938933   0.09938233  -2.7412643   -1.5820438\n",
      "  -8.771377     2.2877958   -4.924851     0.60401803   2.5267403\n",
      "  -4.667606     2.613879     1.2547536   -5.105443    -2.445917\n",
      "   6.992859     1.8155738   -0.04187321   5.805454    -0.4838178\n",
      "   1.7930386    0.20750862  -1.2521837   -0.9675909   -6.9135284\n",
      "  -8.615692     0.9610581   -6.886037     4.5576835   -4.2044277 ]\n"
     ]
    }
   ],
   "source": [
    "vec_king = model.wv['king']\n",
    "print(vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert integer input to vectors for RNN using one hot encoding\n",
    "\n",
    "def encode(X,seq_len, vocab_size):\n",
    "    x = np.zeros((len(X),seq_len, vocab_size), dtype=np.float32)\n",
    "    for ind,batch in enumerate(X):\n",
    "        for j, elem in enumerate(batch):\n",
    "            x[ind, j, elem] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to generate infinite-stream of inputs for training\n",
    "\n",
    "def batch_gen(batch_size=32, seq_len=10, max_no=100):\n",
    "    # Randomly generate a batch of integer sequences (X) and its sorted counterpart (Y)\n",
    "    x = np.zeros((batch_size, seq_len, max_no), dtype=np.float32)\n",
    "    y = np.zeros((batch_size, seq_len, max_no), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "\t# Generates a batch of input\n",
    "        X = np.random.randint(max_no, size=(batch_size, seq_len))\n",
    "\n",
    "        Y = np.sort(X, axis=1)\n",
    "\n",
    "        for ind,batch in enumerate(X):\n",
    "            for j, elem in enumerate(batch):\n",
    "                x[ind, j, elem] = 1\n",
    "\n",
    "        for ind,batch in enumerate(Y):\n",
    "            for j, elem in enumerate(batch):\n",
    "                y[ind, j, elem] = 1\n",
    "\n",
    "        yield x, y\n",
    "        x.fill(0.0)\n",
    "        y.fill(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_len, max_no, n_layers, hidden_size):\n",
    "    model = Sequential()\n",
    "    # the encoder LSTM\n",
    "    model.add(LSTM(hidden_size, input_shape=(seq_len, max_no)))\n",
    "    # in next layer, repeat the input seq_len times\n",
    "    model.add(RepeatVector(seq_len))\n",
    "    # decoder RNN, which will return output sequence\n",
    "    for _ in range(n_layers):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(max_no)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 23:23:10.641355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-15 23:23:10.641482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.641551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.641622: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.641738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.642116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.642346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.642406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.642505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kacem/usr/lib64\n",
      "2021-12-15 23:23:10.642516: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-15 23:23:10.642961: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Global parameters.\n",
    "batch_size=32\n",
    "seq_len = 15 # number of elements in sequence to sort\n",
    "max_no = 100 # upper range of the numbers in sequence\n",
    "hidden_size = 128\n",
    "n_layers = 2\n",
    "epochs = 100000\n",
    "\n",
    "model = create_model(seq_len, max_no, n_layers, hidden_size)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : epochs\n",
      "0.0062500000931322575 : accuracy\n",
      "[ 2  3  8 15 21 23 23 46 59 60 60 68 73 84 87] : sorted by NumPy algorithm\n",
      "[56 56 56 56 56 56 56 56 56 56 46 46 46 46 46] : sorted by trained RNN\n",
      "\n",
      "\n",
      "500 : epochs\n",
      "0.10000000149011612 : accuracy\n",
      "[10 13 15 17 37 38 53 58 60 63 78 82 88 90 94] : sorted by NumPy algorithm\n",
      "[ 0  8 15 24 31 40 50 58 63 67 73 78 85 96 99] : sorted by trained RNN\n",
      "\n",
      "\n",
      "1000 : epochs\n",
      "0.12083332985639572 : accuracy\n",
      "[ 2  5  8 10 25 28 39 41 47 49 65 73 81 87 88] : sorted by NumPy algorithm\n",
      "[ 0  5  9 14 20 31 36 38 44 49 66 71 81 86 90] : sorted by trained RNN\n",
      "\n",
      "\n",
      "1500 : epochs\n",
      "0.23749999701976776 : accuracy\n",
      "[ 3 22 24 26 37 44 53 54 55 58 72 80 85 94 94] : sorted by NumPy algorithm\n",
      "[ 6 20 25 28 35 45 51 53 56 58 71 78 83 93 98] : sorted by trained RNN\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7094/85598695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# We'll test RNN after each 500 iteration to check how well it is performing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                                                     class_weight)\n\u001b[1;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-project-env-conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now the training loop, we'll sample input batches from the generator function written previously and feed it to the RNN for learning\n",
    "\n",
    "for ind,(X,Y) in enumerate(batch_gen(batch_size, seq_len, max_no)):\n",
    "    loss, acc = model.train_on_batch(X, Y)\n",
    "    # We'll test RNN after each 500 iteration to check how well it is performing\n",
    "    if ind % 500 == 0:\n",
    "        testX = np.random.randint(max_no, size=(1, seq_len))\n",
    "        test = encode(testX, seq_len, max_no)\n",
    "        #print(testX, ': input sequence')\n",
    "        print(ind, ': epochs')\n",
    "        print(acc, ': accuracy')\n",
    "        loss_str = '{:4.3}'.format(loss)\n",
    "        acc_str = '{:4.3}'.format(acc)\n",
    "        \n",
    "        y = model.predict(test, batch_size=1)\n",
    "        np_sorted = np.sort(testX)[0]\n",
    "        rnn_sorted = np.argmax(y, axis=2)[0]\n",
    "        is_equal = np.array_equal(np_sorted, rnn_sorted)\n",
    "        print(np_sorted, ': sorted by NumPy algorithm')\n",
    "        print(rnn_sorted, ': sorted by trained RNN')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if acc > 0.995 :\n",
    "            model.save('model.h5')\n",
    "            break\n",
    "        \n",
    "    if ind > epochs:\n",
    "        # Save trained model\n",
    "        model.save('model.h5')\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
